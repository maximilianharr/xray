{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @todo description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "imgfile = '/media/docker/workspace/xraybones/res/xray.jpg'\n",
    "imgfile = '/media/docker/workspace/xraybones/res/xray2.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def convolve2D(image: np.array([]), kernel: np.array([]), padding=0, strides=1) -> np.array([]):\n",
    "    # Cross Correlation\n",
    "    kernel = np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    # Gather Shapes of Kernel + Image + Padding\n",
    "    xKernShape = kernel.shape[0]\n",
    "    yKernShape = kernel.shape[1]\n",
    "    xImgShape = image.shape[0]\n",
    "    yImgShape = image.shape[1]\n",
    "\n",
    "    # Shape of Output Convolution\n",
    "    xOutput = int(((xImgShape - xKernShape + 2 * padding) / strides) + 1)\n",
    "    yOutput = int(((yImgShape - yKernShape + 2 * padding) / strides) + 1)\n",
    "    output = np.zeros((xOutput, yOutput))\n",
    "\n",
    "    # Apply Equal Padding to All Sides\n",
    "    if padding != 0:\n",
    "        imagePadded = np.zeros((image.shape[0] + padding*2, image.shape[1] + padding*2))\n",
    "        imagePadded[int(padding):int(-1 * padding), int(padding):int(-1 * padding)] = image\n",
    "        print(imagePadded)\n",
    "    else:\n",
    "        imagePadded = image\n",
    "\n",
    "    # Iterate through image\n",
    "    for y in range(image.shape[1]):\n",
    "        # Exit Convolution\n",
    "        if y > image.shape[1] - yKernShape:\n",
    "            break\n",
    "        # Only Convolve if y has gone down by the specified Strides\n",
    "        if y % strides == 0:\n",
    "            for x in range(image.shape[0]):\n",
    "                # Go to next row once kernel is out of bounds\n",
    "                if x > image.shape[0] - xKernShape:\n",
    "                    break\n",
    "                try:\n",
    "                    # Only Convolve if x has moved by the specified Strides\n",
    "                    if x % strides == 0:\n",
    "                        output[x, y] = (kernel * imagePadded[x: x + xKernShape, y: y + yKernShape]).sum()\n",
    "                except:\n",
    "                    break\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XRay Hand class\n",
    "class ImageType(Enum):\n",
    "    NONE, RAW = -1, 1\n",
    "    \n",
    "class BodyPartType(Enum):\n",
    "    NONE, HAND = -1, 0\n",
    "\n",
    "class BoneDetector():\n",
    "    \n",
    "    figsize = 15\n",
    "    bodyparttype = BodyPartType.NONE\n",
    "    image = np.array([])\n",
    "    images = pd.DataFrame(columns=['img', 'imagetype', 'description'])\n",
    "    \n",
    "    def __init__(self, imagefile: str) -> None:\n",
    "        \"\"\"\n",
    "        Init X-Ray Hand detector\n",
    "            :param imagefile\n",
    "        \"\"\"\n",
    "        # Read image\n",
    "        self.image = cv2.imread(imagefile)\n",
    "        if self.image is None:\n",
    "            raise Exception(\"Could not read the image.\")\n",
    "        \n",
    "        self.image = cv2.cvtColor(src=self.image, code=cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        self.images = self.images.append({'img': self.image, 'imagetype': ImageType.RAW, 'description': 'raw image'}, ignore_index=True)\n",
    "            \n",
    "    def show_all_images(self) -> None:\n",
    "        \"\"\"\n",
    "        Show all images\n",
    "            :param imagetype\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(figsize,figsize))\n",
    "        \n",
    "        for img in self.images.iterrows():\n",
    "            plt.imshow(img.img, cmap='gray')\n",
    "    \n",
    "    def show_image(self, imagetype: ImageType) -> None:\n",
    "        \"\"\"\n",
    "        Show specified image\n",
    "            :param imagetype\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(self.figsize,self.figsize))\n",
    "        \n",
    "        for index, row in self.images.iterrows():\n",
    "            if row.imagetype == imagetype:\n",
    "                plt.imshow(row.img, cmap='gray')\n",
    "    \n",
    "    def show_image_overlay(self, imagetype: ImageType) -> None:\n",
    "        \"\"\"\n",
    "        Overlay image with imagetype over raw image\n",
    "            :param imagetype\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(self.figsize,self.figsize))\n",
    "        \n",
    "        image_raw_rgb = cv2.cvtColor(src=self.image, code=cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        for index, row in self.images.iterrows():\n",
    "            if row.imagetype == imagetype:\n",
    "                image_raw_r = cv2.cvtColor(src=row.img, code=cv2.COLOR_GRAY2RGB)\n",
    "                image_raw_r[:, :, 1] = 0\n",
    "                image_raw_r[:, :, 2] = 0\n",
    "        \n",
    "        #\n",
    "        vis = cv2.addWeighted(image_raw_r,1.0,image_raw_rgb,1.0,0)\n",
    "        \n",
    "        plt.imshow(vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init bone detector\n",
    "bonedetector = BoneDetector(imgfile)\n",
    "bonedetector.show_image(ImageType.RAW)\n",
    "#bonedetector.show_image_overlay(ImageType.RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-quilt",
   "metadata": {},
   "source": [
    "## Use k-means to remove background color intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means color clustering\n",
    "#Read the original image grayscale color\n",
    "img = cv2.imread(imgfile, 0) \n",
    "\n",
    "# Get image height and width\n",
    "rows, cols = img.shape[:]\n",
    "\n",
    "# Image two-dimensional pixel conversion to one-dimensional\n",
    "data = img.reshape((rows * cols))\n",
    "data = np.float32(data)\n",
    "\n",
    "# Definition Center (type,max_iter,epsilon)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS +\n",
    "            cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "\n",
    "# K-Means clustering into 4 categories\n",
    "compactness, labels, centers = cv2.kmeans(data, 2, None, criteria, 10, flags)\n",
    "\n",
    "# Generate final image\n",
    "res = centers[labels.flatten()]\n",
    "dst = res.reshape((img.shape[0],img.shape[1]))\n",
    "\n",
    "print('First cluster intensity %f' % min(centers))\n",
    "\n",
    "# Display image\n",
    "#fig = plt.figure(figsize=(figsize,figsize))\n",
    "#plt.imshow(dst, 'gray')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop image\n",
    "imgraw = cv.imread(imgfile)\n",
    "\n",
    "cropping = 0.08\n",
    "imgraw = imgraw[ int(cropping*imgraw.shape[0]):int((1.-cropping)*imgraw.shape[0]), \n",
    "          int(cropping*imgraw.shape[1]):int((1.-cropping)*imgraw.shape[1])]\n",
    "imgraw = cv2.cvtColor(src=imgraw, code=cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-royalty",
   "metadata": {},
   "source": [
    "## Hand and Finger Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract convex hull and defects\n",
    "# https://medium.com/analytics-vidhya/hand-detection-and-finger-counting-using-opencv-python-5b594704eb08\n",
    "# https://docs.opencv.org/3.4/dd/d49/tutorial_py_contour_features.html\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Loading exposure images into a list\n",
    "img = imgraw.copy()\n",
    "\n",
    "# Blur image\n",
    "img_blur = cv2.GaussianBlur(img,(15,15),0)\n",
    "\n",
    "# Threshold (binary image)\n",
    "low_thres = int(min(centers))\n",
    "ret,img_thres = cv2.threshold(img_blur, low_thres, 255, cv2.THRESH_BINARY)\n",
    "img_eq = cv2.equalizeHist(img_thres)\n",
    "\n",
    "plt.subplot(1,3,1); plt.xticks([]),plt.yticks([]); plt.imshow(img_eq, cmap='gray')\n",
    "\n",
    "# Morphological (remove outliers)\n",
    "kernel = np.ones((15, 15), 'uint8')\n",
    "img_morph = cv2.morphologyEx(img_eq, cv2.MORPH_CLOSE, kernel)\n",
    "#equalize = cv2.morphologyEx(equalize, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "plt.subplot(1,3,2); plt.xticks([]),plt.yticks([]); plt.imshow(img_morph, cmap='gray')\n",
    "\n",
    "# Contours\n",
    "contours, hierarchy = cv2.findContours(img_morph, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = max(contours, key=lambda x: cv2.contourArea(x))\n",
    "\n",
    "# Convex hull\n",
    "hull = cv2.convexHull(contours)\n",
    "\n",
    "# Hull defects\n",
    "hull = cv2.convexHull(contours, returnPoints=False)\n",
    "defects = cv2.convexityDefects(contours, hull)\n",
    "\n",
    "img2 = imgraw.copy()\n",
    "cv2.drawContours(img2, [contours], -1, (255, 255, 255), 2)\n",
    "\n",
    "plt.subplot(1,3,3); plt.xticks([]),plt.yticks([]); plt.imshow(img2, cmap='gray')\n",
    "\n",
    "# Extract hand\n",
    "#value = 240\n",
    "#mask = img > value\n",
    "#img_new = np.where((255 - img) < value, 255, img+value)\n",
    "#plt.imshow(img_new, cmap='gray')\n",
    "\n",
    "#kernel = np.ones((10, 10), 'uint8')\n",
    "#dilate_img = cv2.dilate(img_new, kernel, iterations=3)\n",
    "#erosion_img = cv2.erode(dilate_img, kernel, iterations=3)\n",
    "#opening = cv2.morphologyEx(img_new, cv2.MORPH_OPEN, kernel)\n",
    "#closing = cv2.morphologyEx(img_new, cv2.MORPH_CLOSE, kernel)\n",
    "#gradient = cv2.morphologyEx(closing, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "\n",
    "#img_list = [cv.imread(fn) for fn in img_fn]\n",
    "#exposure_time = np.array(15.0, dtype=np.float32)\n",
    "#merge_debevec = cv.createMergeDebevec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = imgraw.copy()\n",
    "\n",
    "if defects is not None:\n",
    "    defcnt = 0\n",
    "\n",
    "fingergaps = np.array([])\n",
    "\n",
    "for i in range(defects.shape[0]):  # calculate the angle\n",
    "    s, e, f, d = defects[i][0]\n",
    "    start = tuple(contours[s][0])\n",
    "    end = tuple(contours[e][0])\n",
    "    far = tuple(contours[f][0])\n",
    "    a = np.sqrt((end[0] - start[0]) ** 2 + (end[1] - start[1]) ** 2)\n",
    "    b = np.sqrt((far[0] - start[0]) ** 2 + (far[1] - start[1]) ** 2)\n",
    "    c = np.sqrt((end[0] - far[0]) ** 2 + (end[1] - far[1]) ** 2)\n",
    "    angle = np.arccos((b ** 2 + c ** 2 - a ** 2) / (2 * b * c))  #      cosine theorem\n",
    "    if angle <= np.pi / 2:  # angle less than 90 degree, treat as fingers\n",
    "        defcnt += 1\n",
    "        cv.circle(img2, far, 10, [255, 0, 0], -1)\n",
    "        fingergaps = np.append(fingergaps, [far[0], -far[1]])\n",
    "        \n",
    "fingergaps = fingergaps.flatten().reshape(-1,2)\n",
    "\n",
    "if defcnt > 0:\n",
    "    defcnt = defcnt+1\n",
    "    \n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "cv.putText(img2, '%d Fingers detected' % defcnt, (0, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0) , 2, cv.LINE_AA)\n",
    "cv2.drawContours(img2, [contours], -1, (255, 255, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convex hull and defects\n",
    "img2 = imgraw.copy()\n",
    "\n",
    "cnt = contours\n",
    "\n",
    "defects_arr = np.array([])\n",
    "\n",
    "for i in range(defects.shape[0]):\n",
    "    s,e,f,d = defects[i,0]\n",
    "    start = tuple(cnt[s][0])\n",
    "    end = tuple(cnt[e][0])\n",
    "    far = tuple(cnt[f][0])\n",
    "    \n",
    "    if d > 500:\n",
    "        continue\n",
    "    elif far[1] > 0.9*img2.shape[0]: # ignore points at bottom\n",
    "        continue\n",
    "    defects_arr = np.append(defects_arr, [far[0], -far[1]])\n",
    "    #cv.circle(img2,far,10,[255,0,0],-1)\n",
    "\n",
    "# K-Means Clustering of fingertip defects\n",
    "defects_arr = defects_arr.reshape(-1,2)\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(defects_arr)\n",
    "kmeans.labels_\n",
    "fingertips = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "cv.putText(img2, '%d Fingers detected' % defcnt, (0, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0) , 2, cv.LINE_AA)\n",
    "cv2.drawContours(img2, [contours], -1, (255, 255, 255), 2)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "plt.imshow(img2, cmap='gray')\n",
    "\n",
    "for fingergap in fingergaps:\n",
    "    plt.scatter(fingergap[0], -fingergap[1], c='r', marker='.', s=200)\n",
    "    \n",
    "for fingertip in fingertips:\n",
    "    plt.scatter(fingertip[0], -fingertip[1], c='r', marker='x', s=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-pickup",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "moral-tract",
   "metadata": {},
   "source": [
    "# I do not know what I'm doing here\n",
    "epsilon = 0.04*cv.arcLength(cnt,True)\n",
    "approx = cv2.approxPolyDP(cnt,epsilon,True)\n",
    "pnts = approx.flatten().reshape(-1,2)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.plot(pnts[:,0],pnts[:,1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-spain",
   "metadata": {},
   "source": [
    "## Apply HDR\n",
    "https://docs.opencv.org/3.4/d2/df0/tutorial_py_hdr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HDR\n",
    "# \n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# Loading exposure images into a list\n",
    "img = cv2.imread(imgfile)\n",
    "value=-50\n",
    "img2 = np.where((255 - img) < value, 255, img+value)\n",
    "value=100\n",
    "img3 = np.where((255 - img) < value, 255, img+value)\n",
    "\n",
    "img_list = [img, img2, img3]\n",
    "exposure_times = np.array([10., 15., 20.], dtype=np.float32)\n",
    "\n",
    "# Merge exposures to HDR image\n",
    "merge_debevec = cv2.createMergeDebevec()\n",
    "hdr_debevec = merge_debevec.process(img_list, times=exposure_times.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-limitation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "third-edinburgh",
   "metadata": {},
   "source": [
    "## Detect Shapes\n",
    "https://www.pyimagesearch.com/2016/02/08/opencv-shape-detection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-clerk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "timely-customer",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "https://realpython.com/k-means-clustering-python/\n",
    "https://scikit-learn.org/stable/modules/clustering.html#clustering\n",
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Means histogram clusterin\n",
    "img = imgraw.copy()\n",
    "\n",
    "# Get image height and width\n",
    "rows, cols = img.shape[:]\n",
    "\n",
    "# Image two-dimensional pixel conversion to one-dimensional\n",
    "data = img.reshape((rows * cols))\n",
    "data = np.float32(data)\n",
    "\n",
    "# Definition Center (type,max_iter,epsilon)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS +\n",
    "            cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "\n",
    "# K-Means clustering into 4 categories\n",
    "compactness, labels, centers = cv2.kmeans(data, 2, None, criteria, 10, flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(centers)\n",
    "hist,bins = np.histogram(img.ravel(),256,[0,256])\n",
    "\n",
    "#plt.hist(image.ravel(),256,[0,256]); \n",
    "plt.plot(bins[1:],hist,[0,256])\n",
    "plt.scatter(centers, np.zeros(len(centers)), c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-baltimore",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = imgraw.copy()\n",
    "\n",
    "ret,image = ret,image = cv2.threshold(image, int(centers[1]), 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#equalize= cv2.equalizeHist(thresh_image)\n",
    "\n",
    "#canny_image = cv2.Canny(equalize,250,255)\n",
    "#canny_image = cv2.convertScaleAbs(canny_image)\n",
    "#kernel = np.ones((3,3), np.uint8)\n",
    "#dilated_image = cv2.dilate(canny_image,kernel,iterations=1)\n",
    "\n",
    "#contours, hierarchy = cv2.findContours(dilated_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#contours= sorted(contours, key = cv2.contourArea, reverse = True)[:10]\n",
    "#c=contours[0]\n",
    "\n",
    "#final = cv2.drawContours(img, [c], -1, (255,0, 0), 3)\n",
    "#mask = np.zeros(image.shape,np.uint8)\n",
    "\n",
    "#new_image = cv2.drawContours(mask,[c],0,255,-1,)\n",
    "#new_image = cv2.bitwise_and(image, image, mask=equalize)\n",
    "\n",
    "#cv2.namedWindow(\"new\",cv2.WINDOW_NORMAL)\n",
    "#cv2.imshow(\"new\",new_image)\n",
    "#cv2.waitKey(0)\n",
    "\n",
    "# Visualize data\n",
    "\n",
    "fig = plt.figure(figsize=(figsize,figsize))\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-filename",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "forward-script",
   "metadata": {},
   "source": [
    "## Image Filtering\n",
    "https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-option",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-glasgow",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "https://medium.com/analytics-vidhya/2d-convolution-using-python-numpy-43442ff5f381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-medicaid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means color clustering\n",
    "#Read the original image grayscale color\n",
    "img = imgraw.copy()\n",
    "\n",
    "#Get image height and width\n",
    "rows, cols = img.shape[:]\n",
    "\n",
    "#Image two-dimensional pixel conversion to one-dimensional\n",
    "data = img.reshape((rows * cols))\n",
    "data = np.float32(data)\n",
    "\n",
    "#Definition Center (type,max_iter,epsilon)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS +\n",
    "            cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "\n",
    "#K-Means clustering into 4 categories\n",
    "compactness, labels, centers = cv2.kmeans(data, 2, None, criteria, 10, flags)\n",
    "\n",
    "#Generate final image\n",
    "res = centers[labels.flatten()]\n",
    "dst = res.reshape((img.shape[0],img.shape[1]))\n",
    "\n",
    "#Display image\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(figsize,figsize))\n",
    "\n",
    "plt.imshow(dst, 'gray')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image coordinates as points with threshold\n",
    "plt.figure(figsize=(figsize, figsize))\n",
    "image = imgraw.copy()\n",
    "indices = np.where(image > 180)\n",
    "plt.axis('equal')\n",
    "plt.scatter(indices[1], -indices[0], marker='.', c='k', s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-surgery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display mutliple images\n",
    "titles = [u'The original image', u'Cluster image']  \n",
    "images = [img, dst]  \n",
    "for i in range(2):  \n",
    "    plt.subplot(1,2,i+1), plt.imshow(images[i], 'gray'), \n",
    "    plt.title(titles[i])  \n",
    "    plt.xticks([]),plt.yticks([])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-novelty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-general",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "embedded-queen",
   "metadata": {},
   "source": [
    "## Image Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-french",
   "metadata": {},
   "source": [
    "Links:\n",
    "https://heartbeat.fritz.ai/opencv-python-cheat-sheet-from-importing-images-to-face-detection-52919da36433\n",
    "https://www.kdnuggets.com/2019/08/introduction-image-segmentation-k-means-clustering.html\n",
    "https://docs.opencv.org/3.4/d8/dbc/tutorial_histogram_calculation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and Convert color\n",
    "image = cv2.imread('/media/docker/artifacts/imgs/xray.jpg') \n",
    "image = cv2.cvtColor(src=image, code=cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Crop\n",
    "    cropped = image[10:500, 500:2000]\n",
    "    # Convert Color\n",
    "    #image = cv2.cvtColor(src=image, code=cv2.COLOR_BGR2GRAY) \n",
    "    # Blur\n",
    "    image = cv2.blur(image, ksize=(30, 30))\n",
    "    # Flip\n",
    "    image = cv2.flip(image, flipCode=1)\n",
    "    # Resize\n",
    "    scale=0.5\n",
    "    image = cv2.resize(image, (int(image.shape[1]*scale), int(image.shape[0]*scale)) , interpolation = cv2.INTER_AREA)\n",
    "    # Canny Edge\n",
    "    image = cv2.Canny(image,150,200)\n",
    "    # Morphological (dilate, erode, open, close, gradient)\n",
    "    image = cv2.dilate(image, kernel, iterations=3)\n",
    "    image = cv2.erode(image, kernel, iterations=3)\n",
    "    image = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "    image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "    image = cv2.morphologyEx(image, cv2.MORPH_GRADIENT, kernel)\n",
    "    # Threshold\n",
    "    # https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html\n",
    "    ret,image = cv2.threshold(img, 10, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "# Histogram\n",
    "hist,bins = np.histogram(image.ravel(),256,[0,256])\n",
    "plt.plot(bins[1:],hist,[0,256])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(figsize,figsize))\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-shopping",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
