{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @todo description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "figsmall = 5\n",
    "figbig = 15\n",
    "imgfile = '/media/docker/workspace/xraybones/res/xray2.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-modern",
   "metadata": {},
   "source": [
    "## Normalize Images (Size, Histogram, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgraw = cv2.imread(imgfile)\n",
    "imgraw = cv2.cvtColor(src=imgraw, code=cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert to grayscale\n",
    "imgeq = imgraw.copy()\n",
    "\n",
    "# Crop image\n",
    "cropping = 0.08\n",
    "imgeq = imgeq[ int(cropping*imgeq.shape[0]):int((1.-cropping)*imgeq.shape[0]), \n",
    "          int(cropping*imgeq.shape[1]):int((1.-cropping)*imgeq.shape[1])]\n",
    "\n",
    "# Scale to width: 1000\n",
    "[height, width] = imgeq.shape\n",
    "scale=1000./width\n",
    "imgeq = cv2.resize(imgeq, \n",
    "                   (int(imgeq.shape[1]*scale), int(imgeq.shape[0]*scale)), \n",
    "                   interpolation = cv2.INTER_AREA)\n",
    "\n",
    "# Spread histogram along whole image\n",
    "imgeq=cv2.equalizeHist(src=imgeq)\n",
    "\n",
    "# Visualize results\n",
    "fig = plt.figure(figsize=(figbig, figsmall));\n",
    "plt.subplot(131);\n",
    "\n",
    "hist=cv2.calcHist(imgraw,[0],None,[256],[0,256])\n",
    "plt.plot(hist)\n",
    "hist=cv2.calcHist(imgeq,[0],None,[256],[0,256])\n",
    "plt.plot(hist)\n",
    "plt.legend(['old image', 'new image'])\n",
    "\n",
    "plt.subplot(132);\n",
    "plt.xticks([]),plt.yticks([]); \n",
    "plt.imshow(imgraw, cmap='gray')\n",
    "\n",
    "\n",
    "plt.subplot(133);\n",
    "plt.xticks([]),plt.yticks([]); \n",
    "plt.imshow(imgeq, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-royalty",
   "metadata": {},
   "source": [
    "## Hand and Finger Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract convex hull and defects\n",
    "# https://medium.com/analytics-vidhya/hand-detection-and-finger-counting-using-opencv-python-5b594704eb08\n",
    "# https://docs.opencv.org/3.4/dd/d49/tutorial_py_contour_features.html\n",
    "fig = plt.figure(figsize=(figbig, figbig))\n",
    "\n",
    "# Loading exposure images into a list\n",
    "img = imgeq.copy()\n",
    "\n",
    "# Blur image\n",
    "img_blur = cv2.GaussianBlur(img,(15,15),0)\n",
    "\n",
    "# Threshold (binary image)\n",
    "low_thres = np.average(imgeq)\n",
    "ret,img_thres = cv2.threshold(img_blur, low_thres, 255, cv2.THRESH_BINARY)\n",
    "img_eq = cv2.equalizeHist(img_thres)\n",
    "\n",
    "plt.subplot(1,3,1); plt.xticks([]),plt.yticks([]); plt.imshow(img_eq, cmap='gray')\n",
    "\n",
    "# Morphological (remove outliers)\n",
    "kernel = np.ones((15, 15), 'uint8')\n",
    "img_morph = cv2.morphologyEx(img_eq, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "plt.subplot(1,3,2); plt.xticks([]),plt.yticks([]); plt.imshow(img_morph, cmap='gray')\n",
    "\n",
    "# Contours\n",
    "contours, hierarchy = cv2.findContours(img_morph, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = max(contours, key=lambda x: cv2.contourArea(x))\n",
    "\n",
    "# Convex hull\n",
    "hull = cv2.convexHull(contours)\n",
    "\n",
    "# Hull defects\n",
    "hull = cv2.convexHull(contours, returnPoints=False)\n",
    "defects = cv2.convexityDefects(contours, hull)\n",
    "\n",
    "img2 = imgeq.copy()\n",
    "cv2.drawContours(img2, [contours], -1, (255, 255, 255), 2)\n",
    "\n",
    "plt.subplot(1,3,3); plt.xticks([]),plt.yticks([]); plt.imshow(img2, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = imgeq.copy()\n",
    "\n",
    "if defects is not None:\n",
    "    defcnt = 0\n",
    "\n",
    "fingergaps = np.array([])\n",
    "\n",
    "for i in range(defects.shape[0]):  # calculate the angle\n",
    "    s, e, f, d = defects[i][0]\n",
    "    start = tuple(contours[s][0])\n",
    "    end = tuple(contours[e][0])\n",
    "    far = tuple(contours[f][0])\n",
    "    a = np.sqrt((end[0] - start[0]) ** 2 + (end[1] - start[1]) ** 2)\n",
    "    b = np.sqrt((far[0] - start[0]) ** 2 + (far[1] - start[1]) ** 2)\n",
    "    c = np.sqrt((end[0] - far[0]) ** 2 + (end[1] - far[1]) ** 2)\n",
    "    angle = np.arccos((b ** 2 + c ** 2 - a ** 2) / (2 * b * c))  #      cosine theorem\n",
    "    if angle <= np.pi / 2:  # angle less than 90 degree, treat as fingers\n",
    "        defcnt += 1\n",
    "        cv2.circle(img2, far, 10, [255, 0, 0], -1)\n",
    "        fingergaps = np.append(fingergaps, [far[0], -far[1]])\n",
    "        \n",
    "fingergaps = fingergaps.flatten().reshape(-1,2)\n",
    "\n",
    "if defcnt > 0:\n",
    "    defcnt = defcnt+1\n",
    "    \n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(figbig, figbig))\n",
    "\n",
    "cv2.putText(img2, '%d Fingers detected' % defcnt, (0, 50), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0) , 2, cv2.LINE_AA)\n",
    "cv2.drawContours(img2, [contours], -1, (255, 255, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convex hull and defects\n",
    "img2 = imgeq.copy()\n",
    "\n",
    "cnt = contours\n",
    "\n",
    "defects_arr = np.array([])\n",
    "\n",
    "for i in range(defects.shape[0]):\n",
    "    s,e,f,d = defects[i,0]\n",
    "    start = tuple(cnt[s][0])\n",
    "    end = tuple(cnt[e][0])\n",
    "    far = tuple(cnt[f][0])\n",
    "    \n",
    "    if d > 500:\n",
    "        continue\n",
    "    elif far[1] > 0.9*img2.shape[0]: # ignore points at bottom\n",
    "        continue\n",
    "    defects_arr = np.append(defects_arr, [far[0], -far[1]])\n",
    "    #cv.circle(img2,far,10,[255,0,0],-1)\n",
    "\n",
    "# K-Means Clustering of fingertip defects\n",
    "defects_arr = defects_arr.reshape(-1,2)\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(defects_arr)\n",
    "kmeans.labels_\n",
    "fingertips = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop Hand contour\n",
    "imghand = imgeq.copy()\n",
    "\n",
    "mask = np.zeros_like(imghand) # Create mask where white is what we want, black otherwise\n",
    "cv2.drawContours(image=mask, contours=[contours], \n",
    "                 contourIdx=0, color=(255, 255, 255), thickness=-1) # Draw filled contour in mask\n",
    "imghand = np.zeros_like(imghand) # Extract out the object and place into output image\n",
    "imghand[mask == 255] = img[mask == 255]\n",
    "\n",
    "# Show the output image\n",
    "fig = plt.figure(figsize=(figsmall,figsmall))\n",
    "plt.imshow(imghand, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig = plt.figure(figsize=(figbig, figbig))\n",
    "\n",
    "cv2.putText(img2, '%d Fingers detected' % defcnt, (0, 50), \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0) , 2, cv2.LINE_AA)\n",
    "cv2.drawContours(img2, [contours], -1, (255, 255, 255), 2)\n",
    "\n",
    "fig = plt.figure(figsize=(figbig, figbig))\n",
    "plt.imshow(imghand, cmap='gray')\n",
    "\n",
    "for fingergap in fingergaps:\n",
    "    plt.scatter(fingergap[0], -fingergap[1], c='r', marker='.', s=200)\n",
    "    \n",
    "for fingertip in fingertips:\n",
    "    plt.scatter(fingertip[0], -fingertip[1], c='r', marker='x', s=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-cookie",
   "metadata": {},
   "source": [
    "## Image Convolution With Existing Image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "opened-honolulu",
   "metadata": {},
   "source": [
    "kernel = cv2.imread('/media/docker/workspace/xraybones/res/finger2.png')\n",
    "kernel = cv2.cvtColor(src=kernel, code=cv2.COLOR_BGR2GRAY)\n",
    "kernel = kernel/2555.\n",
    "kernel = kernel-np.mean(kernel)\n",
    "\n",
    "plt.figure(figsize=(figsmall,figsmall))\n",
    "plt.imshow(kernel, cmap='gray')\n",
    "\n",
    "dst = cv2.filter2D(imgeq, -1, kernel)\n",
    "\n",
    "fig = plt.figure(figsize=(figbig,figbig))\n",
    "plt.imshow(dst, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-score",
   "metadata": {},
   "source": [
    "## Bone Detection (Canny Edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canny Edge Detector\n",
    "img = imghand.copy()\n",
    "img_blur = cv2.blur(img, (6,6))\n",
    "img_canny = cv2.Canny(img_blur, 0, 30, 100)\n",
    "\n",
    "# draw hand contour overlay\n",
    "cv2.drawContours(image=img_canny, contours=[contours], \n",
    "                 contourIdx=0, color=(0,0,0), thickness=5)\n",
    "\n",
    "fig = plt.figure(figsize=(figbig,figbig))\n",
    "plt.imshow(img_canny, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize image\n",
    "image_raw_r = cv2.cvtColor(src=img_canny, code=cv2.COLOR_GRAY2RGB)*100\n",
    "image_raw_r[:, :, 0] = 0\n",
    "image_raw_r[:, :, 2] = 0\n",
    "\n",
    "img = imghand.copy()\n",
    "img = cv2.cvtColor(src=img, code=cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "\n",
    "# Visualize data\n",
    "fig = plt.figure(figsize=(figbig,figbig))\n",
    "vis = cv2.addWeighted(img, 0.3, image_raw_r, 1.0,0)\n",
    "\n",
    "for fingergap in fingergaps:\n",
    "    plt.scatter(fingergap[0], -fingergap[1], c='r', marker='.', s=200)\n",
    "    \n",
    "for fingertip in fingertips:\n",
    "    plt.scatter(fingertip[0], -fingertip[1], c='r', marker='x', s=200)\n",
    "\n",
    "plt.imshow(vis, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-occasions",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "comparable-privacy",
   "metadata": {},
   "source": [
    "# Legacy Code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "technological-highland",
   "metadata": {},
   "source": [
    "# XRay Hand class\n",
    "class ImageType(Enum):\n",
    "    NONE, RAW = -1, 1\n",
    "    \n",
    "class BodyPartType(Enum):\n",
    "    NONE, HAND = -1, 0\n",
    "\n",
    "class BoneDetector():\n",
    "    \n",
    "    figsize = 15\n",
    "    bodyparttype = BodyPartType.NONE\n",
    "    image = np.array([])\n",
    "    images = pd.DataFrame(columns=['img', 'imagetype', 'description'])\n",
    "    \n",
    "    def __init__(self, imagefile: str) -> None:\n",
    "        \"\"\"\n",
    "        Init X-Ray Hand detector\n",
    "            :param imagefile\n",
    "        \"\"\"\n",
    "        # Read image\n",
    "        self.image = cv2.imread(imagefile)\n",
    "        if self.image is None:\n",
    "            raise Exception(\"Could not read the image.\")\n",
    "        \n",
    "        self.image = cv2.cvtColor(src=self.image, code=cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        self.images = self.images.append({'img': self.image, 'imagetype': ImageType.RAW, 'description': 'raw image'}, ignore_index=True)\n",
    "            \n",
    "    def show_all_images(self) -> None:\n",
    "        \"\"\"\n",
    "        Show all images\n",
    "            :param imagetype\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(self.figsize,self.figsize))\n",
    "        \n",
    "        for img in self.images.iterrows():\n",
    "            plt.imshow(img.img, cmap='gray')\n",
    "    \n",
    "    def show_image(self, imagetype: ImageType) -> None:\n",
    "        \"\"\"\n",
    "        Show specified image\n",
    "            :param imagetype\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(self.figsize,self.figsize))\n",
    "        \n",
    "        for index, row in self.images.iterrows():\n",
    "            if row.imagetype == imagetype:\n",
    "                plt.imshow(row.img, cmap='gray')\n",
    "    \n",
    "    def show_image_overlay(self, imagetype: ImageType) -> None:\n",
    "        \"\"\"\n",
    "        Overlay image with imagetype over raw image\n",
    "            :param imagetype\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(self.figsize,self.figsize))\n",
    "        \n",
    "        image_raw_rgb = cv2.cvtColor(src=self.image, code=cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        for index, row in self.images.iterrows():\n",
    "            if row.imagetype == imagetype:\n",
    "                image_raw_r = cv2.cvtColor(src=row.img, code=cv2.COLOR_GRAY2RGB)\n",
    "                image_raw_r[:, :, 1] = 0\n",
    "                image_raw_r[:, :, 2] = 0\n",
    "        \n",
    "        #\n",
    "        vis = cv2.addWeighted(image_raw_r,1.0,image_raw_rgb,1.0,0)\n",
    "        \n",
    "        plt.imshow(vis)\n",
    "\n",
    "# Init bone detector\n",
    "bonedetector = BoneDetector(imgfile)\n",
    "#bonedetector.show_image(ImageType.RAW)\n",
    "#bonedetector.show_image_overlay(ImageType.RAW)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "closing-accused",
   "metadata": {},
   "source": [
    "from __future__ import print_function\n",
    "import cv2 as cv\n",
    "\n",
    "max_lowThreshold = 100\n",
    "window_name = 'Edge Map'\n",
    "title_trackbar = 'Min Threshold:'\n",
    "ratio = 3\n",
    "kernel_size = 3\n",
    "\n",
    "def CannyThreshold(val):\n",
    "    low_threshold = val\n",
    "    img_blur = cv.blur(src_gray, (5,5))\n",
    "    detected_edges = cv.Canny(img_blur, low_threshold, low_threshold*ratio, kernel_size)\n",
    "    mask = detected_edges != 0\n",
    "    dst = src * (mask[:,:,None].astype(src.dtype))\n",
    "    return dst\n",
    "    \n",
    "src = cv2.imread(imgfile)\n",
    "src_gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "#cv.namedWindow(window_name)\n",
    "#cv.createTrackbar(title_trackbar, window_name , 0, max_lowThreshold, CannyThreshold)\n",
    "dst = CannyThreshold(0)\n",
    "#cv.waitKey()\n",
    "\n",
    "fig = plt.figure(figsize=(figbig,figbig))\n",
    "plt.imshow(dst, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.Canny?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "moral-tract",
   "metadata": {},
   "source": [
    "# I do not know what I'm doing here\n",
    "epsilon = 0.04*cv.arcLength(cnt,True)\n",
    "approx = cv2.approxPolyDP(cnt,epsilon,True)\n",
    "pnts = approx.flatten().reshape(-1,2)\n",
    "\n",
    "fig = plt.figure(figsize=(figbig, figbig))\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.plot(pnts[:,0],pnts[:,1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-spain",
   "metadata": {},
   "source": [
    "## Apply HDR\n",
    "https://docs.opencv.org/3.4/d2/df0/tutorial_py_hdr.html"
   ]
  },
  {
   "cell_type": "raw",
   "id": "silent-commander",
   "metadata": {},
   "source": [
    "# Apply HDR\n",
    "# \n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# Loading exposure images into a list\n",
    "img = cv2.imread(imgfile)\n",
    "value=-50\n",
    "img2 = np.where((255 - img) < value, 255, img+value)\n",
    "value=100\n",
    "img3 = np.where((255 - img) < value, 255, img+value)\n",
    "\n",
    "img_list = [img, img2, img3]\n",
    "exposure_times = np.array([10., 15., 20.], dtype=np.float32)\n",
    "\n",
    "# Merge exposures to HDR image\n",
    "merge_debevec = cv2.createMergeDebevec()\n",
    "hdr_debevec = merge_debevec.process(img_list, times=exposure_times.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-limitation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "third-edinburgh",
   "metadata": {},
   "source": [
    "## Detect Shapes\n",
    "https://www.pyimagesearch.com/2016/02/08/opencv-shape-detection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-clerk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "timely-customer",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "https://realpython.com/k-means-clustering-python/\n",
    "https://scikit-learn.org/stable/modules/clustering.html#clustering\n",
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "built-guyana",
   "metadata": {},
   "source": [
    "# k-Means histogram clusterin\n",
    "img = imgeq.copy()\n",
    "\n",
    "# Get image height and width\n",
    "rows, cols = img.shape[:]\n",
    "\n",
    "# Image two-dimensional pixel conversion to one-dimensional\n",
    "data = img.reshape((rows * cols))\n",
    "data = np.float32(data)\n",
    "\n",
    "# Definition Center (type,max_iter,epsilon)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS +\n",
    "            cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "\n",
    "# K-Means clustering into 4 categories\n",
    "compactness, labels, centers = cv2.kmeans(data, 2, None, criteria, 10, flags)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "elder-creator",
   "metadata": {},
   "source": [
    "print(centers)\n",
    "hist,bins = np.histogram(img.ravel(),256,[0,256])\n",
    "\n",
    "#plt.hist(image.ravel(),256,[0,256]); \n",
    "plt.plot(bins[1:],hist,[0,256])\n",
    "plt.scatter(centers, np.zeros(len(centers)), c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-script",
   "metadata": {},
   "source": [
    "## Image Filtering\n",
    "https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-option",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-glasgow",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "https://medium.com/analytics-vidhya/2d-convolution-using-python-numpy-43442ff5f381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-toddler",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "political-quilt",
   "metadata": {},
   "source": [
    "## Use k-means to remove background color intensity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "statewide-appliance",
   "metadata": {},
   "source": [
    "# k-means color clustering\n",
    "#Read the original image grayscale color\n",
    "img = imgeq.copy()\n",
    "\n",
    "# Get image height and width\n",
    "rows, cols = img.shape[:]\n",
    "\n",
    "# Image two-dimensional pixel conversion to one-dimensional\n",
    "data = img.reshape((rows * cols))\n",
    "data = np.float32(data)\n",
    "\n",
    "# Definition Center (type,max_iter,epsilon)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS +\n",
    "            cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "\n",
    "# K-Means clustering into 2 categories\n",
    "compactness, labels, centers = cv2.kmeans(data, 5, None, criteria, 10, flags)\n",
    "\n",
    "#Visualize\n",
    "fig = plt.figure(figsize=(figsmall, figsmall));\n",
    "hist=cv2.calcHist(img,[0],None,[256],[0,256])\n",
    "plt.plot(hist)\n",
    "plt.scatter(centers, np.zeros(len(centers)), c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-surgery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-general",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "embedded-queen",
   "metadata": {},
   "source": [
    "## Image Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-french",
   "metadata": {},
   "source": [
    "Links:\n",
    "https://heartbeat.fritz.ai/opencv-python-cheat-sheet-from-importing-images-to-face-detection-52919da36433\n",
    "https://www.kdnuggets.com/2019/08/introduction-image-segmentation-k-means-clustering.html\n",
    "https://docs.opencv.org/3.4/d8/dbc/tutorial_histogram_calculation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-shopping",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
